{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOF9oal81kJojE0WbwWRB4Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/binayak9932/python_scripts/blob/main/diversion_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeAW5FNZEwDh",
        "outputId": "a43b2fd5-65e1-4c19-e27a-8e826ebf4732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHlr9BLxFMV2",
        "outputId": "e7c4d0e8-3112-411f-e9a7-ad256bafd121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `'stopwords''\n",
            "/bin/bash: -c: line 1: `nltk.download('stopwords')'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nabnumaFl97",
        "outputId": "2a71cab6-65e0-4742-df11-ea7e94269087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "\n",
        "# Sample complaints\n",
        "complaints = [\n",
        "    \"The internet is too slow!\",\n",
        "    \"There is a leak in the kitchen!\",\n",
        "    \"My computer crashed and I lost all my work.\",\n",
        "    \"The elevator is not working properly.\",\n",
        "    \"I received a wrong item in my order.\"\n",
        "]\n",
        "\n",
        "# Define complaint categories and their keywords\n",
        "categories = {\n",
        "    'Internet': ['internet', 'slow'],\n",
        "    'Maintenance': ['leak', 'elevator', 'not working'],\n",
        "    'Technical': ['computer', 'crashed', 'lost', 'work'],\n",
        "    'Order Issue': ['wrong', 'item', 'order']\n",
        "}\n",
        "\n",
        "# Tokenize and preprocess complaints\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Categorize complaints and assign priorities\n",
        "def categorize_complaints(complaints, categories):\n",
        "    categorized_complaints = defaultdict(list)\n",
        "    for complaint in complaints:\n",
        "        tokens = preprocess_text(complaint)\n",
        "        priority = 'Low'  # Default priority\n",
        "        for category, keywords in categories.items():\n",
        "            if any(keyword in tokens for keyword in keywords):\n",
        "                categorized_complaints[category].append(complaint)\n",
        "                if priority != 'High':  # If priority is not already set to High\n",
        "                    priority = 'High'\n",
        "        categorized_complaints['Priority'].append(priority)\n",
        "    return dict(categorized_complaints)\n",
        "\n",
        "categorized = categorize_complaints(complaints, categories)\n",
        "for category, complaints in categorized.items():\n",
        "    if category != 'Priority':\n",
        "        print(f\"Category: {category}\")\n",
        "        print(\"Complaints:\", complaints)\n",
        "        print()\n",
        "\n",
        "print(\"Priorities:\", categorized['Priority'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpjcd-6IFooq",
        "outputId": "fdf62932-7f37-4f2a-b6ce-8cb598a8cf92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category: Internet\n",
            "Complaints: ['The internet is too slow!']\n",
            "\n",
            "Category: Maintenance\n",
            "Complaints: ['There is a leak in the kitchen!', 'The elevator is not working properly.']\n",
            "\n",
            "Category: Technical\n",
            "Complaints: ['My computer crashed and I lost all my work.']\n",
            "\n",
            "Category: Order Issue\n",
            "Complaints: ['I received a wrong item in my order.']\n",
            "\n",
            "Priorities: ['High', 'High', 'High', 'High', 'High']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Define the data\n",
        "categories_data = {\n",
        "    'Internet': ['internet', 'slow'],\n",
        "    'Maintenance': ['leak', 'elevator', 'not working'],\n",
        "    'Technical': ['computer', 'crashed', 'lost', 'work'],\n",
        "    'Order Issue': ['wrong', 'item', 'order']\n",
        "}\n",
        "\n",
        "# Convert dictionary data into a list of dictionaries for CSV writing\n",
        "data_for_csv = [{'Category': category, 'Keywords': ', '.join(keywords)} for category, keywords in categories_data.items()]\n",
        "\n",
        "# Write data to a CSV file\n",
        "with open('categories.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['Category', 'Keywords']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data_for_csv)\n",
        "\n",
        "print(\"CSV file 'categories.csv' has been created with the categories data.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDeK7sopG4qI",
        "outputId": "af1ff4eb-e21f-4394-df5d-926f0e17e009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file 'categories.csv' has been created with the categories data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def read_categories_from_csv(file_name):\n",
        "    categories = {}\n",
        "    with open(file_name, 'r', encoding='utf-8-sig') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        for row in reader:\n",
        "            category = row['Category']\n",
        "            keywords = row['Keywords'].split(',')\n",
        "            categories[category] = [keyword.strip().lower() for keyword in keywords]\n",
        "    return categories\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def categorize_complaints(complaint, categories):\n",
        "    tokens = preprocess_text(complaint)\n",
        "    priority = 'Low'\n",
        "    categorized_complaints = defaultdict(list)\n",
        "    for category, keywords in categories.items():\n",
        "        if any(keyword in tokens for keyword in keywords):\n",
        "            categorized_complaints[category].append(complaint)\n",
        "            if priority != 'High':\n",
        "                priority = 'High'\n",
        "    return categorized_complaints, priority\n",
        "\n",
        "\n",
        "categories = read_categories_from_csv('categories.csv')\n",
        "\n",
        "\n",
        "user_complaint = input(\"Enter your complaint: \")\n",
        "\n",
        "\n",
        "categorized_complaints, priority = categorize_complaints(user_complaint, categories)\n",
        "\n",
        "\n",
        "if priority == 'High':\n",
        "    print(\"Priority: High\")\n",
        "else:\n",
        "    print(\"Priority: Low\")\n",
        "\n",
        "print(\"Categorized Complaints:\")\n",
        "for category, complaints in categorized_complaints.items():\n",
        "    print(f\"Category: {category}\")\n",
        "    print(\"Complaints:\", complaints)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gGyRXVBGLsn",
        "outputId": "4aa2f562-e3fb-4735-816d-10120e7a9e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your complaint: leak in pipe in garage\n",
            "Priority: High\n",
            "Categorized Complaints:\n",
            "Category: Maintenance\n",
            "Complaints: ['leak in pipe in garage']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "\n",
        "categories_data = {\n",
        "    'Internet': {'Keywords': ['internet' ], 'Priority': 'Low'},\n",
        "    'Maintenance': {'Keywords': ['leak', 'elevator', 'not working'], 'Priority': 'High'},\n",
        "    'Technical': {'Keywords': ['computer', 'crashed', 'lost', 'work'], 'Priority': 'Low'},\n",
        "    'Order Issue': {'Keywords': ['wrong', 'item', 'order'], 'Priority': 'Low'}\n",
        "}\n",
        "\n",
        "\n",
        "data_for_csv = [{'Category': category, 'Keywords': ', '.join(details['Keywords']), 'Priority': details['Priority']} for category, details in categories_data.items()]\n",
        "\n",
        "\n",
        "with open('categories_with_priority.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['Category', 'Keywords', 'Priority']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data_for_csv)\n",
        "\n",
        "print(\"CSV file 'categories_with_priority.csv' has been created with the categories data including Priority.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHRyUIRoH1os",
        "outputId": "6a2ccd98-add9-4e40-99a9-351b7dc113b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file 'categories_with_priority.csv' has been created with the categories data including Priority.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def read_categories_from_csv(file_name):\n",
        "    categories = {}\n",
        "    with open(file_name, 'r', encoding='utf-8-sig') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        for row in reader:\n",
        "            category = row['Category']\n",
        "            keywords = row['Keywords'].split(',')\n",
        "            priority = row['Priority']\n",
        "            categories[category] = {'Keywords': [keyword.strip().lower() for keyword in keywords], 'Priority': priority}\n",
        "    return categories\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def categorize_complaints(complaint, categories):\n",
        "    tokens = preprocess_text(complaint)\n",
        "    priority = 'Low'\n",
        "    categorized_complaints = defaultdict(list)\n",
        "    for category, details in categories.items():\n",
        "        keywords = details['Keywords']\n",
        "        if any(keyword in tokens for keyword in keywords):\n",
        "            categorized_complaints[category].append(complaint)\n",
        "            if details['Priority'] == 'High':\n",
        "                priority = 'High'\n",
        "    return categorized_complaints, priority\n",
        "\n",
        "\n",
        "categories = read_categories_from_csv('categories_with_priority.csv')\n",
        "\n",
        "\n",
        "user_complaint = input(\"Enter your complaint: \")\n",
        "\n",
        "categorized_complaints, priority = categorize_complaints(user_complaint, categories)\n",
        "\n",
        "\n",
        "if priority == 'High':\n",
        "    print(\"Priority: High\")\n",
        "else:\n",
        "    print(\"Priority: Low\")\n",
        "\n",
        "print(\"Categorized Complaints:\")\n",
        "for category, complaints in categorized_complaints.items():\n",
        "    print(f\"Category: {category}\")\n",
        "    print(\"Complaints:\", complaints)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktwwdOsXH21A",
        "outputId": "40056e4f-9ee3-40c7-d7f1-ad1ccf16ac40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your complaint: elevator very slow in my apartment\n",
            "Priority: High\n",
            "Categorized Complaints:\n",
            "Category: Maintenance\n",
            "Complaints: ['elevator very slow in my apartment']\n"
          ]
        }
      ]
    }
  ]
}